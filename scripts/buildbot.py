"""
Builds sumatra and uploads results to s3 for easy analysis, viewable at:
http://kjkpub.s3.amazonaws.com/sumatrapdf/buildbot/index.html
"""
import os, os.path, shutil, sys, time, re, string

from util import log, run_cmd_throw, run_cmd, test_for_flag, s3UploadFilePublic
from util import s3UploadDataPublic, ensure_s3_doesnt_exist, ensure_path_exists
from util import parse_svninfo_out, s3List, s3Delete, verify_started_in_right_directory

def skip_error(s):
	if "C2220" in s: return True # warning treated as error
	return False

# given a text generated by compiler, extract the lines that contain
# error information
def extract_compile_errors(s):
	errors = []
	for l in s.split('\n'):
		if ": error " in l or ": warning " in l:
			if not skip_error(l) and l not in errors:
				errors.append(l)
	return errors

g_src_files = None
def get_src_files():
	global g_src_files
	if g_src_files is not None: return g_src_files
	g_src_files = []
	for root, dirs, files in os.walk("src"):
		for file in files:
			file_path = os.path.join(root, file)
			g_src_files.append(file_path)
	return g_src_files

g_src_trans_map = None
def get_src_trans_map():
	global g_src_trans_map
	if g_src_trans_map is not None: return g_src_trans_map
	g_src_trans_map = {}
	for f in get_src_files():
		g_src_trans_map[f.lower()] = f
	return g_src_trans_map

# for some reason file names are lower-cased and the url has exact case
# we need to convert src_path to have the exact case for urls to work
# i.e. given "src\doc.h" we need to return "src\Doc.h"
def trans_src_path(s):
	return get_src_trans_map()[s]

def a(url, txt): return '<a href="' + url + '">' + txt + '</a>'

# Turn:
# src\utils\allocator.h(156)
# Into:
# <a href="https://code.google.com/p/sumatrapdf/source/browse/trunk/src/utils/allocator.h#156">src\utils\allocator.h(156)</a>
def htmlize_src_link(s):
	parts = s.split("(")
	src_path = parts[0] # src\utils\allocator.h
	src_path = trans_src_path(src_path) # src\utils\Allocator.h
	src_path_in_url = src_path.replace("\\", "/")
	src_line = parts[1][:-1] # "156)"" => "156"
	url = "https://code.google.com/p/sumatrapdf/source/browse/trunk/" + src_path_in_url + "#" + src_line
	return a(url, src_path + "(" + src_line + ")")

# Turn:
# c:\users\kkowalczyk\src\sumatrapdf\src\utils\allocator.h(156) : warning C6011: Dereferencing NULL pointer 'node'. : Lines: 149, 150, 151, 153, 154, 156
# Into:
# <a href="https://code.google.com/p/sumatrapdf/source/browse/trunk/src/utils/allocator.h#156">src\utils\allocator.h(156)</a>:<br>
# warning C6011: Dereferencing NULL pointer 'node'. : Lines: 149, 150, 151, 153, 154, 156
def htmlize_error_lines(lines):
	if len(lines) == 0: return []
	res = []
	rel_path_start = lines[0].find("sumatrapdf\\") + len("sumatrapdf\\")
	for l in lines:
		l = l[rel_path_start:]
		err_start = l.find(" : ")
		src = l[:err_start]
		msg = l[err_start + 3:]
		a = htmlize_src_link(src)
		s = a + " " + msg
		res.append(s)
	return res

def pre(s): return '<pre style="white-space: pre-wrap;">' + s + '</pre>'

def gen_errors_html(errors, ver):
	s = "<html><body>"
	s += "There were %d warnings and errors in Sumatra build %s" % (len(errors), str(ver))
	s += pre(string.join(errors, ""))
	s += "</pre>"
	s += "</body></html>"
	return s

class Stats:
	def __init__(self):
		self.analyze_warnings_count = 0
		self.analyze_out = None

	def add_field(self, name, val):
		self.fields.append("%s: %s" % (name, str(val)))

	def to_s(self):
		self.fields = []
		self.add_field("AnalyzeWarningsCount", self.analyze_warnings_count)
		return string.join(self.fields, "\n")

def get_cache_dir():
	cache_dir = os.path.join("..", "sumatrapdfcache", "buildbot")
	if not os.path.exists(cache_dir): os.makedirs(cache_dir)
	return cache_dir

def s3UploadDataPublicWithContentType(data, s3_path):
	# writing to a file to force boto to set Content-Type based on file extension.
	# TODO: there must be a simpler way
	tmp_name = os.path.basename(s3_path)
	tmp_path = os.path.join(get_cache_dir(), tmp_name)
	open(tmp_path, "w").write(data)
	s3UploadFilePublic(tmp_path, s3_path)
	os.remove(tmp_path)

# return true if we already have results for a given build number in s3
def has_already_been_built(ver):
	s3_dir = "sumatrapdf/buildbot/"
	expected_name = s3_dir + ver + "/analyze.html"
	keys = s3List(s3_dir)
	for k in keys:
		if k.name == expected_name: return True
	return False

# build sumatrapdf/buildbot/index.html summary page that links to each 
# sumatrapdf/buildbot/${ver}/analyze.html
# TODO: download stats.txt files locally
# TODO: add summary stats from stats.txt
def build_index_html():
	s3_dir = "sumatrapdf/buildbot/"
	html = "<html><body>\n"
	html += "<p>SumatraPDF buildbot results:</p>\n"
	html += "<ul>\n"
	keys = s3List(s3_dir)
	for k in keys:
		#print(k.name)
		name = k.name[len(s3_dir):]
		print(name)
		if name.endswith("/analyze.html"):
			parts = name.split("/")
			ver = parts[0]
			url = "http://kjkpub.s3.amazonaws.com/" + s3_dir + name
			html += "  <li>Build " + a(url, ver) + "</li>\n"
	html += "</ul>\n"
	html += "</body></html>\n"
	#print(html)
	s3UploadDataPublicWithContentType(html, "sumatrapdf/buildbot/index.html")

def build_analyze(stats, ver):
	config = "CFG=rel"
	obj_dir = "obj-rel"
	shutil.rmtree(obj_dir, ignore_errors=True)
	extcflags = "EXTCFLAGS=-DSVN_PRE_RELEASE_VER=%s" % ver
	platform = "PLATFORM=X86"

	# disable ucrt because vs2012 doesn't support it and I give it priority
	# over 2010 (if both installed) hoping it has better /analyze
	(out, err, errcode) = run_cmd("nmake", "-f", "makefile.msvc", "WITH_SUM_ANALYZE=yes", "WITH_UCRT=no", config, extcflags, platform, "all_sumatrapdf")
	stats.analyze_out = out

# returns local and latest (on the server) svn versions
def get_svn_versions():
	(out, err) = run_cmd_throw("svn", "info")
	ver_local = str(parse_svninfo_out(out))
	(out, err) = run_cmd_throw("svn", "info", "https://sumatrapdf.googlecode.com/svn/trunk")
	ver_latest = str(parse_svninfo_out(out))
	return ver_local, ver_latest

def svn_update_to_ver(ver):
	run_cmd_throw("svn", "update", "-r" + ver)

# TODO: more build types (regular 32bit release and debug, 64bit?)
def build_version(ver):
	print("Building version %s" % ver)
	svn_update_to_ver(ver)
	stats = Stats()
	build_analyze(stats, ver)

	out = stats.analyze_out
	errors = htmlize_error_lines(extract_compile_errors(out))
	html = gen_errors_html(errors, ver)
	stats.analyze_warnings_count = len(errors)
	stats_txt = stats.to_s()
	s3dir = "sumatrapdf/buildbot/%s/" % ver
	s3UploadDataPublicWithContentType(html, s3dir + "analyze.html")
	s3UploadDataPublicWithContentType(stats_txt, s3dir + "stats.txt")

	build_index_html()

def buildbot_loop():
	while True:
		(local_ver, latest_ver) = get_svn_versions()
		print("local ver: %s, latest ver: %s" % (local_ver, latest_ver))
		while int(local_ver) < int(latest_ver):
			if not has_already_been_built(local_ver):
				build_version(local_ver)
			else:
				print("We have already built revision %s" % local_ver)
			local_ver = str(int(local_ver)+1)
		print("Sleeping for 15 minutes")
		time.sleep(60*15) # 15 mins

def main():
	verify_started_in_right_directory()
	# to avoid problems, we build a separate source tree, just for the buildbot
	src_path = os.path.join("..", "sumatrapdf_buildbot")
	ensure_path_exists(src_path)
	os.chdir(src_path)
	buildbot_loop()

if __name__ == "__main__":
	main()
