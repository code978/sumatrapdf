"""
Builds Sumatra with /analyze flag and upload results to s3 for easy analysis.
"""
import os, os.path, shutil, sys, time, re, string

from util import log, run_cmd_throw, run_cmd, test_for_flag, s3UploadFilePublic
from util import s3UploadDataPublic, ensure_s3_doesnt_exist, ensure_path_exists
from util import parse_svninfo_out, s3List, s3Delete

def skip_error(s):
	if "C2220" in s: return True # warning treated as error
	return False

# given a text generated by compiler, extract the lines that contain
# error information
def extract_compile_errors(s):
	errors = []
	for l in s.split('\n'):
		if ": error " in l or ": warning " in l:
			if not skip_error(l) and l not in errors:
				errors.append(l)
	return errors

g_src_files = None
def get_src_files():
	global g_src_files
	if g_src_files is not None: return g_src_files
	g_src_files = []
	for root, dirs, files in os.walk("src"):
		for file in files:
			file_path = os.path.join(root, file)
			g_src_files.append(file_path)
	return g_src_files

g_src_trans_map = None
def get_src_trans_map():
	global g_src_trans_map
	if g_src_trans_map is not None: return g_src_trans_map
	g_src_trans_map = {}
	for f in get_src_files():
		g_src_trans_map[f.lower()] = f
	return g_src_trans_map

# for some reason file names are lower-cased and the url has exact case
# we need to convert src_path to have the exact case for urls to work
# i.e. given "src\doc.h" we need to return "src\Doc.h"
def trans_src_path(s):
	return get_src_trans_map()[s]

# Turn:
# src\utils\allocator.h(156)
# Into:
# <a href="https://code.google.com/p/sumatrapdf/source/browse/trunk/src/utils/allocator.h#156">src\utils\allocator.h(156)</a>
def htmlize_src_link(s):
	parts = s.split("(")
	src_path = parts[0] # src\utils\allocator.h
	src_path = trans_src_path(src_path) # src\utils\Allocator.h
	src_path_in_url = src_path.replace("\\", "/")
	src_line = parts[1][:-1] # "156)"" => "156"
	href = "https://code.google.com/p/sumatrapdf/source/browse/trunk/" + src_path_in_url + "#" + src_line
	return '<a href="' + href + '">' + src_path + "(" + src_line + ")" + '</a>'

# Turn:
# c:\users\kkowalczyk\src\sumatrapdf\src\utils\allocator.h(156) : warning C6011: Dereferencing NULL pointer 'node'. : Lines: 149, 150, 151, 153, 154, 156
# Into:
# <a href="https://code.google.com/p/sumatrapdf/source/browse/trunk/src/utils/allocator.h#156">src\utils\allocator.h(156)</a>:<br>
# warning C6011: Dereferencing NULL pointer 'node'. : Lines: 149, 150, 151, 153, 154, 156
def htmlize_error_lines(lines):
	if len(lines) == 0: return []
	res = []
	rel_path_start = lines[0].find("sumatrapdf\\") + len("sumatrapdf\\")
	for l in lines:
		l = l[rel_path_start:]
		err_start = l.find(" : ")
		src = l[:err_start]
		msg = l[err_start + 3:]
		a = htmlize_src_link(src)
		s = a + ":\n" + msg
		res.append(s)
	return res

def pre(s): return "<pre>" + s + "</pre>"

def gen_errors_html(errors, ver):
	s = "<html><body>"
	s += "There were %d warnings and errors in Sumatra build %s" % (len(errors), str(ver))
	s += pre(string.join(errors, ""))
	s += "</pre>"
	s += "</body></html>"
	return s

def test_extract_compile_errors():
	s = open("analyze-6665-out.txt").read()
	errors = extract_compile_errors(s)
	errors = htmlize_error_lines(errors)
	html = gen_errors_html(errors, "6665")
	open("analyze.html", "w").write(html)

# TODO: do this in a loop, wake up every 1hr, update svn to see if there were changes
# and if there were, do the build
# TODO: more build types (regular 32bit release and debug, 64bit?)
def main():
	(out, err) = run_cmd_throw("svn", "info")
	ver = str(parse_svninfo_out(out))
	# TODO: check if results of this already exist on s3, abort if they do
	print("Building revision %s" % ver)

	config = "CFG=rel"
	obj_dir = "obj-rel"
	shutil.rmtree(obj_dir, ignore_errors=True)
	extcflags = "EXTCFLAGS=-DSVN_PRE_RELEASE_VER=%s" % ver
	platform = "PLATFORM=X86"

	# disable ucrt because vs2012 doesn't support it and I give it priority
	# over 2010 (if both installed) hoping it has better /analyze
	(out, err, errcode) = run_cmd("nmake", "-f", "makefile.msvc", "WITH_SUM_ANALYZE=yes", "WITH_UCRT=no", config, extcflags, platform, "all_sumatrapdf")
	open("analyze-%s-out.txt" % ver, "w").write(out)
	open("analyze-%s-err.txt" % ver, "w").write(err)

	errors = extract_compile_errors(out)
	errors = htmlize_error_lines(errors)
	html = gen_errors_html(errors, ver)

	# TODO:
	# - generate analyze.json with basic stats (how many analyze errors)
	# - upload html and json to s3 kjkpub bucket /sumatrapdf/buildbot/${ver}/analyze.html and analyze.json
	# - generate /sumatrapdf/buildbot/index.html that links to all ${ver}/analyze.html and shows the summary
	#   it gets from all ${ver}/analyze.json

if __name__ == "__main__":
	test_extract_compile_errors()
	sys.exit(1)
	main()
